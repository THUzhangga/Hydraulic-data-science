{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler #only can be imported above pytorch0.2.0\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool0 = nn.AvgPool2d((2, 4), padding=1)\n",
    "        self.conv1 = nn.Conv2d(1, 5, 4)\n",
    "        self.pool1 = nn.AvgPool2d(3)\n",
    "        self.conv2 = nn.Conv2d(5, 5, 3)\n",
    "        self.pool2 = nn.MaxPool2d(3)\n",
    "        self.fc1 = nn.Linear(80, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool0(x)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5*4*4)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4  # netCDF4非Python自带包，需要自行下载\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "f = netCDF4.Dataset('D:\\\\sst.mnmean.nc')  # ftp://ftp.cdc.noaa.gov/Datasets/noaa.ersst.v5/sst.mnmean.nc\n",
    "SST = f.variables['sst'][-1203:, :, :].data\n",
    "\n",
    "SST[SST < -2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mean = np.zeros((12, 89, 180))\n",
    "month_s = np.arange(0, 1188, 12)\n",
    "for i in range(12):\n",
    "    month_mean[i] = np.average(SST[month_s + i], axis = 0)\n",
    "    SST[month_s + i] -= month_mean[i]\n",
    "for i in range(12):\n",
    "    SST[1188 + i] -= month_mean[i]\n",
    "for i in range(3):\n",
    "    SST[1200 + i] -= month_mean[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST = SST.reshape((1200, 1, 89, 180))\n",
    "\n",
    "X_train = SST[:-16]\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_valid = SST[-16:-4]\n",
    "X_valid = torch.from_numpy(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = ((134, 44), \\\n",
    "       (94, 44), \\\n",
    "       (57, 37), \\\n",
    "       (39, 44), \\\n",
    "       (21, 36), \\\n",
    "       (9, 16), \\\n",
    "       (172, 59), \\\n",
    "       (158, 28), \\\n",
    "       (65, 64), \\\n",
    "       (0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0465993881225586 0.6835797429084778\n",
      "1 0.9747850298881531 0.6549382209777832\n",
      "2 0.8863504528999329 0.6098104119300842\n",
      "3 0.7840589284896851 0.5596358180046082\n",
      "4 0.6766080260276794 0.5468974709510803\n",
      "5 0.5807129144668579 0.7936881184577942\n",
      "6 0.5363522171974182 1.088138222694397\n",
      "7 0.5773686766624451 0.8834357261657715\n",
      "8 0.615320086479187 0.9258086085319519\n",
      "9 0.5854175686836243 0.7049086093902588\n",
      "10 0.5361378788948059 0.474771648645401\n",
      "11 0.49880969524383545 0.41472306847572327\n",
      "12 0.4840785562992096 0.4199715554714203\n",
      "13 0.48431453108787537 0.4274919927120209\n",
      "14 0.49079370498657227 0.4431491792201996\n",
      "15 0.49424758553504944 0.46046414971351624\n",
      "16 0.4938221871852875 0.4491209089756012\n",
      "17 0.4867117702960968 0.43406984210014343\n",
      "18 0.47634685039520264 0.4229389429092407\n",
      "19 0.46304330229759216 0.4076906144618988\n",
      "20 0.45169714093208313 0.3971867561340332\n",
      "21 0.44332918524742126 0.4074654281139374\n",
      "22 0.4409140646457672 0.38971027731895447\n",
      "23 0.4423217177391052 0.41832205653190613\n",
      "24 0.4440515339374542 0.44110941886901855\n",
      "25 0.4433950185775757 0.404109388589859\n",
      "26 0.43839794397354126 0.43733248114585876\n",
      "27 0.43011271953582764 0.4139150083065033\n",
      "28 0.42216765880584717 0.38777533173561096\n",
      "29 0.41687989234924316 0.4040575921535492\n",
      "30 0.41379767656326294 0.3863620460033417\n",
      "31 0.4118261933326721 0.3788534104824066\n",
      "32 0.40977296233177185 0.38742396235466003\n",
      "33 0.4060767590999603 0.378066748380661\n",
      "34 0.4003705084323883 0.36809656023979187\n",
      "35 0.39391180872917175 0.36967313289642334\n",
      "36 0.3878806233406067 0.34830573201179504\n",
      "37 0.3828873038291931 0.3323054313659668\n",
      "38 0.378866970539093 0.3362675607204437\n",
      "39 0.3746119737625122 0.3167688548564911\n",
      "40 0.3696208894252777 0.33039483428001404\n",
      "41 0.36441153287887573 0.3289472460746765\n",
      "42 0.3594892919063568 0.31267741322517395\n",
      "43 0.3543674051761627 0.3062728941440582\n",
      "44 0.3488781154155731 0.2667122185230255\n",
      "45 0.34327250719070435 0.255297988653183\n",
      "46 0.3384140431880951 0.23645561933517456\n",
      "47 0.3338371515274048 0.24513036012649536\n",
      "48 0.3285519778728485 0.24879275262355804\n",
      "49 0.3233188986778259 0.2799307405948639\n",
      "50 0.31905683875083923 0.26357418298721313\n",
      "51 0.3156101405620575 0.3154396414756775\n",
      "52 0.3137630522251129 0.21575309336185455\n",
      "53 0.318454772233963 0.3949998915195465\n",
      "54 0.326911985874176 0.211586132645607\n",
      "55 0.31207364797592163 0.22960080206394196\n",
      "56 0.30297237634658813 0.3499906063079834\n",
      "57 0.31148579716682434 0.2353302240371704\n",
      "58 0.2979179322719574 0.2183133363723755\n",
      "59 0.3012634515762329 0.3018411099910736\n",
      "60 0.29807764291763306 0.27017509937286377\n",
      "61 0.29241859912872314 0.2106027454137802\n",
      "62 0.29505613446235657 0.23057271540164948\n",
      "63 0.28645431995391846 0.2761131823062897\n",
      "64 0.29034990072250366 0.21944600343704224\n",
      "65 0.28322023153305054 0.20548145473003387\n",
      "66 0.2850490212440491 0.25016579031944275\n",
      "67 0.28092530369758606 0.25437691807746887\n",
      "68 0.279375821352005 0.2109624594449997\n",
      "69 0.2784331142902374 0.22161465883255005\n",
      "70 0.27473247051239014 0.25961926579475403\n",
      "71 0.27556687593460083 0.2263643592596054\n",
      "72 0.2711385488510132 0.20735321938991547\n",
      "73 0.27183645963668823 0.2260286659002304\n",
      "74 0.26823490858078003 0.23033665120601654\n",
      "75 0.2676897943019867 0.20462645590305328\n",
      "76 0.2658817768096924 0.2071986049413681\n",
      "77 0.2635994255542755 0.22835858166217804\n",
      "78 0.26319077610969543 0.21348994970321655\n",
      "79 0.26011961698532104 0.20639163255691528\n",
      "80 0.2598433494567871 0.22683288156986237\n",
      "81 0.2578418254852295 0.2238827347755432\n",
      "82 0.2561744451522827 0.20663999021053314\n",
      "83 0.2555319666862488 0.21719692647457123\n",
      "84 0.2532031238079071 0.22189204394817352\n",
      "85 0.25243625044822693 0.2062169462442398\n",
      "86 0.25123146176338196 0.21189022064208984\n",
      "87 0.2493869811296463 0.22212326526641846\n",
      "88 0.2487395703792572 0.20955175161361694\n",
      "89 0.24725420773029327 0.2124331146478653\n",
      "90 0.24584628641605377 0.2220965474843979\n",
      "91 0.24511685967445374 0.20915724337100983\n",
      "92 0.2436286062002182 0.20930536091327667\n",
      "93 0.2423202097415924 0.21659334003925323\n",
      "94 0.2414320707321167 0.20684869587421417\n",
      "95 0.23995386064052582 0.20863570272922516\n",
      "96 0.2385670691728592 0.2167290598154068\n",
      "97 0.23750200867652893 0.2100881189107895\n",
      "98 0.23612700402736664 0.2136528342962265\n",
      "99 0.23479855060577393 0.22021150588989258\n",
      "100 0.2337714284658432 0.21209390461444855\n",
      "101 0.23252777755260468 0.2165244221687317\n",
      "102 0.2311934530735016 0.2203052043914795\n",
      "103 0.2301102727651596 0.21324729919433594\n",
      "104 0.22898544371128082 0.22072438895702362\n",
      "105 0.22770360112190247 0.22030192613601685\n",
      "106 0.22657841444015503 0.21466033160686493\n",
      "107 0.2256261706352234 0.2213643193244934\n",
      "108 0.22458995878696442 0.21500951051712036\n",
      "109 0.22350043058395386 0.21350757777690887\n",
      "110 0.22252577543258667 0.21829794347286224\n",
      "111 0.22163477540016174 0.2123650312423706\n",
      "112 0.2206965684890747 0.217501699924469\n",
      "113 0.21973560750484467 0.22025223076343536\n",
      "114 0.2188800424337387 0.2174883633852005\n",
      "115 0.2180955559015274 0.22520749270915985\n",
      "116 0.21726703643798828 0.22128944098949432\n",
      "117 0.21643094718456268 0.22134552896022797\n",
      "118 0.21564456820487976 0.2246423214673996\n",
      "119 0.21490448713302612 0.21936780214309692\n",
      "120 0.21415169537067413 0.22609716653823853\n",
      "121 0.21339018642902374 0.22466595470905304\n",
      "122 0.21264228224754333 0.22725962102413177\n",
      "123 0.21190136671066284 0.23186790943145752\n",
      "124 0.21121527254581451 0.22895149886608124\n",
      "125 0.21055351197719574 0.23414520919322968\n",
      "126 0.20989911258220673 0.23068249225616455\n",
      "127 0.20925244688987732 0.23138868808746338\n",
      "128 0.20861800014972687 0.23225225508213043\n",
      "129 0.20800505578517914 0.22870850563049316\n",
      "130 0.20740799605846405 0.23308396339416504\n",
      "131 0.2068053036928177 0.22908639907836914\n",
      "132 0.2061925083398819 0.23350049555301666\n",
      "133 0.20557084679603577 0.231537327170372\n",
      "134 0.20497387647628784 0.23324942588806152\n",
      "135 0.20439837872982025 0.2328682690858841\n",
      "136 0.20384380221366882 0.23022125661373138\n",
      "137 0.2033105492591858 0.23189383745193481\n",
      "138 0.2027917355298996 0.22734706103801727\n",
      "139 0.2022910863161087 0.2312779277563095\n",
      "140 0.20177936553955078 0.22827567160129547\n",
      "141 0.20127224922180176 0.23368985950946808\n",
      "142 0.20078538358211517 0.23290415108203888\n",
      "143 0.20032471418380737 0.2382211685180664\n",
      "144 0.19988369941711426 0.23670955002307892\n",
      "145 0.19944842159748077 0.23929870128631592\n",
      "146 0.1990082561969757 0.2353687733411789\n",
      "147 0.19855554401874542 0.23657236993312836\n",
      "148 0.1980915665626526 0.23186492919921875\n",
      "149 0.19762633740901947 0.2354947328567505\n",
      "150 0.197161465883255 0.2303943634033203\n",
      "151 0.19670182466506958 0.23903147876262665\n",
      "152 0.19626504182815552 0.22850851714611053\n",
      "153 0.19589319825172424 0.2452545166015625\n",
      "154 0.19561494886875153 0.22191758453845978\n",
      "155 0.19550952315330505 0.2584887742996216\n",
      "156 0.1957980990409851 0.20937378704547882\n",
      "157 0.19688430428504944 0.2920300364494324\n",
      "158 0.19950509071350098 0.19682623445987701\n",
      "159 0.2033461183309555 0.3332434594631195\n",
      "160 0.206035315990448 0.20249181985855103\n",
      "161 0.20093759894371033 0.2682625651359558\n",
      "162 0.19333161413669586 0.2732418477535248\n",
      "163 0.1932269036769867 0.21083475649356842\n",
      "164 0.19765232503414154 0.3041791617870331\n",
      "165 0.19613872468471527 0.24125325679779053\n",
      "166 0.19111725687980652 0.22890043258666992\n",
      "167 0.19231164455413818 0.30489301681518555\n",
      "168 0.1946314573287964 0.23415957391262054\n",
      "169 0.1913326233625412 0.24934037029743195\n",
      "170 0.1896759420633316 0.29874667525291443\n",
      "171 0.19183854758739471 0.23517608642578125\n",
      "172 0.19090574979782104 0.26626890897750854\n",
      "173 0.18852676451206207 0.2867240011692047\n",
      "174 0.18933171033859253 0.23700876533985138\n",
      "175 0.18983741104602814 0.27542394399642944\n",
      "176 0.1879853904247284 0.2720893621444702\n",
      "177 0.18754203617572784 0.24043063819408417\n",
      "178 0.18841047585010529 0.2817646563053131\n",
      "179 0.18750505149364471 0.26429811120033264\n",
      "180 0.18640215694904327 0.2477654665708542\n",
      "181 0.18690331280231476 0.28562766313552856\n",
      "182 0.186830073595047 0.25701263546943665\n",
      "183 0.18569524586200714 0.25470277667045593\n",
      "184 0.185550257563591 0.28328776359558105\n",
      "185 0.1858493685722351 0.2550678551197052\n",
      "186 0.1851724088191986 0.26691994071006775\n",
      "187 0.18453989923000336 0.28385549783706665\n",
      "188 0.1846681386232376 0.259903222322464\n",
      "189 0.18453200161457062 0.2829022705554962\n",
      "190 0.1838759034872055 0.28187790513038635\n",
      "191 0.18353822827339172 0.26765987277030945\n",
      "192 0.18357059359550476 0.2895961105823517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 0.18328599631786346 0.27480044960975647\n",
      "194 0.18276703357696533 0.2728077471256256\n",
      "195 0.18256059288978577 0.2880725562572479\n",
      "196 0.18251702189445496 0.2705892026424408\n",
      "197 0.1822158694267273 0.2832965850830078\n",
      "198 0.18179568648338318 0.28649887442588806\n",
      "199 0.1815796047449112 0.2770906388759613\n",
      "200 0.18146997690200806 0.2947790026664734\n",
      "201 0.18121550977230072 0.2852121889591217\n",
      "202 0.18086321651935577 0.28724566102027893\n",
      "203 0.18062308430671692 0.2973720133304596\n",
      "204 0.18047688901424408 0.2842743992805481\n",
      "205 0.1802806854248047 0.2968388497829437\n",
      "206 0.18000203371047974 0.2930295765399933\n",
      "207 0.17974315583705902 0.29091888666152954\n",
      "208 0.1795731633901596 0.3027160167694092\n",
      "209 0.17941376566886902 0.29282739758491516\n",
      "210 0.17918448150157928 0.3013637661933899\n",
      "211 0.17891918122768402 0.30124548077583313\n",
      "212 0.17869751155376434 0.29702091217041016\n",
      "213 0.17851226031780243 0.3069303631782532\n",
      "214 0.17830677330493927 0.2990483343601227\n",
      "215 0.17807386815547943 0.3057812750339508\n",
      "216 0.17782115936279297 0.3049453794956207\n",
      "217 0.1776006519794464 0.3029598891735077\n",
      "218 0.17740024626255035 0.3103114068508148\n",
      "219 0.17720343172550201 0.3042602837085724\n",
      "220 0.17699362337589264 0.31064504384994507\n",
      "221 0.1767703741788864 0.3073261082172394\n",
      "222 0.17655329406261444 0.3069979250431061\n",
      "223 0.17634926736354828 0.31001225113868713\n",
      "224 0.17615793645381927 0.3048652112483978\n",
      "225 0.17597095668315887 0.31070008873939514\n",
      "226 0.17576096951961517 0.30632472038269043\n",
      "227 0.17555442452430725 0.3094417452812195\n",
      "228 0.1753496378660202 0.30890390276908875\n",
      "229 0.1751481145620346 0.30717387795448303\n",
      "230 0.1749463677406311 0.3098461925983429\n",
      "231 0.1747506707906723 0.3052958846092224\n",
      "232 0.17455638945102692 0.3085998296737671\n",
      "233 0.17435607314109802 0.3052915930747986\n",
      "234 0.1741533875465393 0.3074555993080139\n",
      "235 0.17394763231277466 0.30789557099342346\n",
      "236 0.17374630272388458 0.30691519379615784\n",
      "237 0.17354555428028107 0.3095341622829437\n",
      "238 0.17334063351154327 0.3054339587688446\n",
      "239 0.17313791811466217 0.3087068200111389\n",
      "240 0.17294003069400787 0.30378952622413635\n",
      "241 0.17275428771972656 0.30783650279045105\n",
      "242 0.17257343232631683 0.30475926399230957\n",
      "243 0.17238613963127136 0.30913615226745605\n",
      "244 0.17219920456409454 0.3072786331176758\n",
      "245 0.17200955748558044 0.31033769249916077\n",
      "246 0.17182177305221558 0.3092322051525116\n",
      "247 0.17163856327533722 0.31118956208229065\n",
      "248 0.1714591234922409 0.3099413812160492\n",
      "249 0.17127826809883118 0.31101715564727783\n",
      "250 0.1710921823978424 0.30958762764930725\n",
      "251 0.1709134429693222 0.3107917606830597\n",
      "252 0.17073354125022888 0.30903360247612\n",
      "253 0.17055034637451172 0.31141752004623413\n",
      "254 0.17036521434783936 0.30911701917648315\n",
      "255 0.17017608880996704 0.3131437301635742\n",
      "256 0.16998514533042908 0.308803528547287\n",
      "257 0.16979525983333588 0.3155409097671509\n",
      "258 0.16960971057415009 0.30745187401771545\n",
      "259 0.16943366825580597 0.3193114697933197\n",
      "260 0.1692892462015152 0.3049483001232147\n",
      "261 0.16918566823005676 0.3277814984321594\n",
      "262 0.1691880077123642 0.2995162308216095\n",
      "263 0.16938160359859467 0.34489700198173523\n",
      "264 0.17001082003116608 0.28748318552970886\n",
      "265 0.1713850051164627 0.3781205117702484\n",
      "266 0.17421330511569977 0.2710699439048767\n",
      "267 0.17821015417575836 0.4166358709335327\n",
      "268 0.18202875554561615 0.26669788360595703\n",
      "269 0.1799973100423813 0.3671524226665497\n",
      "270 0.17251469194889069 0.308769166469574\n",
      "271 0.16734839975833893 0.2817869484424591\n",
      "272 0.1703469604253769 0.37731337547302246\n",
      "273 0.1739925593137741 0.28075024485588074\n",
      "274 0.17055533826351166 0.31711992621421814\n",
      "275 0.1666879951953888 0.34818676114082336\n",
      "276 0.1687544733285904 0.2793872058391571\n",
      "277 0.17055796086788177 0.34267663955688477\n",
      "278 0.16777922213077545 0.31911972165107727\n",
      "279 0.16596677899360657 0.28942856192588806\n",
      "280 0.16788753867149353 0.35556483268737793\n",
      "281 0.1682308465242386 0.3047569692134857\n",
      "282 0.16584447026252747 0.30721086263656616\n",
      "283 0.1655730903148651 0.35260701179504395\n",
      "284 0.166966050863266 0.2996289134025574\n",
      "285 0.1661558598279953 0.3276149034500122\n",
      "286 0.1646966189146042 0.3414706289768219\n",
      "287 0.1650470793247223 0.303241491317749\n",
      "288 0.16558738052845 0.34415292739868164\n",
      "289 0.16471025347709656 0.3283654451370239\n",
      "290 0.16391988098621368 0.3128703236579895\n",
      "291 0.16428156197071075 0.348359078168869\n",
      "292 0.16440562903881073 0.31598174571990967\n",
      "293 0.16362550854682922 0.3219106197357178\n",
      "294 0.1632055789232254 0.33948734402656555\n",
      "295 0.16346150636672974 0.30974093079566956\n",
      "296 0.16338995099067688 0.33241838216781616\n",
      "297 0.16278217732906342 0.3279394209384918\n",
      "298 0.1624716818332672 0.3135204017162323\n",
      "299 0.16257554292678833 0.3381443917751312\n",
      "300 0.16245104372501373 0.317336767911911\n",
      "301 0.1619955152273178 0.3209509551525116\n",
      "302 0.16172268986701965 0.3317454755306244\n",
      "303 0.16172145307064056 0.3116998076438904\n",
      "304 0.16161468625068665 0.32877880334854126\n",
      "305 0.16126829385757446 0.321836918592453\n",
      "306 0.16097256541252136 0.3166533410549164\n",
      "307 0.16087427735328674 0.3321841359138489\n",
      "308 0.16078726947307587 0.31623944640159607\n",
      "309 0.16054311394691467 0.3265351951122284\n",
      "310 0.16024614870548248 0.3250615894794464\n",
      "311 0.1600436568260193 0.3172449767589569\n",
      "312 0.15992087125778198 0.3296116292476654\n",
      "313 0.15976838767528534 0.31699368357658386\n",
      "314 0.15953905880451202 0.3249887526035309\n",
      "315 0.15928307175636292 0.3227459490299225\n",
      "316 0.15908147394657135 0.31858858466148376\n",
      "317 0.15891298651695251 0.3271194398403168\n",
      "318 0.15874698758125305 0.31834468245506287\n",
      "319 0.1585415154695511 0.32612350583076477\n",
      "320 0.15831197798252106 0.3241244852542877\n",
      "321 0.15809935331344604 0.3225243389606476\n",
      "322 0.15791338682174683 0.32751473784446716\n",
      "323 0.15774179995059967 0.31967875361442566\n",
      "324 0.15755781531333923 0.3256285488605499\n",
      "325 0.15735813975334167 0.3189334571361542\n",
      "326 0.15714888274669647 0.32105621695518494\n",
      "327 0.15694832801818848 0.3210565149784088\n",
      "328 0.15676316618919373 0.318757027387619\n",
      "329 0.15659022331237793 0.3231286108493805\n",
      "330 0.1564146727323532 0.3178085386753082\n",
      "331 0.15622708201408386 0.3240813612937927\n",
      "332 0.1560436338186264 0.3194715976715088\n",
      "333 0.1558607518672943 0.32396337389945984\n",
      "334 0.15567289292812347 0.32097434997558594\n",
      "335 0.15549249947071075 0.3228594660758972\n",
      "336 0.15531271696090698 0.32145199179649353\n",
      "337 0.15513503551483154 0.32136088609695435\n",
      "338 0.15495964884757996 0.3208952248096466\n",
      "339 0.1547834277153015 0.3192891478538513\n",
      "340 0.1546155959367752 0.31902745366096497\n",
      "341 0.15444646775722504 0.31788039207458496\n",
      "342 0.15426592528820038 0.3174380958080292\n",
      "343 0.15407885611057281 0.31692588329315186\n",
      "344 0.15390101075172424 0.314437597990036\n",
      "345 0.1537184864282608 0.31343603134155273\n",
      "346 0.15353991091251373 0.3102751076221466\n",
      "347 0.15335534512996674 0.31018611788749695\n",
      "348 0.15317420661449432 0.3074815273284912\n",
      "349 0.15299610793590546 0.309444785118103\n",
      "350 0.15281575918197632 0.30539944767951965\n",
      "351 0.1526433825492859 0.3066059648990631\n",
      "352 0.15245719254016876 0.30126217007637024\n",
      "353 0.15227673947811127 0.30502188205718994\n",
      "354 0.15210211277008057 0.30051279067993164\n",
      "355 0.15192988514900208 0.3069455623626709\n",
      "356 0.15175965428352356 0.29796820878982544\n",
      "357 0.1516011506319046 0.30877813696861267\n",
      "358 0.15149278938770294 0.2902143895626068\n",
      "359 0.15149608254432678 0.3163488507270813\n",
      "360 0.15176071226596832 0.2798979878425598\n",
      "361 0.15264762938022614 0.3431057631969452\n",
      "362 0.1549195647239685 0.26659366488456726\n",
      "363 0.15973426401615143 0.4119085371494293\n",
      "364 0.16883963346481323 0.2662982642650604\n",
      "365 0.1756190061569214 0.42496076226234436\n",
      "366 0.1723376214504242 0.2677234411239624\n",
      "367 0.1549552083015442 0.2723030745983124\n",
      "368 0.1519656628370285 0.36553820967674255\n",
      "369 0.16268086433410645 0.2608882486820221\n",
      "370 0.15825258195400238 0.2884571850299835\n",
      "371 0.14974236488342285 0.33934080600738525\n",
      "372 0.15539602935314178 0.2632916271686554\n",
      "373 0.15633469820022583 0.30272915959358215\n",
      "374 0.14942674338817596 0.322844535112381\n",
      "375 0.15172286331653595 0.260516881942749\n",
      "376 0.15435360372066498 0.3034335970878601\n",
      "377 0.1495838612318039 0.30269941687583923\n",
      "378 0.14934803545475006 0.26155218482017517\n",
      "379 0.15224511921405792 0.30837175250053406\n",
      "380 0.14944761991500854 0.2929593324661255\n",
      "381 0.1480768918991089 0.26220473647117615\n",
      "382 0.15035642683506012 0.30160048604011536\n",
      "383 0.148921400308609 0.2851223051548004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384 0.1473223716020584 0.2667428255081177\n",
      "385 0.14896926283836365 0.3070680797100067\n",
      "386 0.14825516939163208 0.2888113260269165\n",
      "387 0.1467156857252121 0.2749788761138916\n",
      "388 0.1477700173854828 0.30958208441734314\n",
      "389 0.1476530134677887 0.2895793616771698\n",
      "390 0.14622369408607483 0.2831544280052185\n",
      "391 0.14677470922470093 0.3147099018096924\n",
      "392 0.1470494419336319 0.29177483916282654\n",
      "393 0.14587847888469696 0.29103267192840576\n",
      "394 0.14580053091049194 0.3136032521724701\n",
      "395 0.14629392325878143 0.2900228202342987\n",
      "396 0.14554639160633087 0.2943248450756073\n",
      "397 0.14511269330978394 0.3095169961452484\n",
      "398 0.1455204039812088 0.28800180554389954\n",
      "399 0.14522916078567505 0.29858991503715515\n",
      "400 0.14461293816566467 0.30427733063697815\n",
      "401 0.1446652114391327 0.2897147834300995\n",
      "402 0.14477141201496124 0.3049187958240509\n",
      "403 0.14433437585830688 0.30090177059173584\n",
      "404 0.14404179155826569 0.29287612438201904\n",
      "405 0.1441570371389389 0.3082398772239685\n",
      "406 0.14404381811618805 0.29848915338516235\n",
      "407 0.14366166293621063 0.2991251051425934\n",
      "408 0.14353778958320618 0.30912265181541443\n",
      "409 0.143577441573143 0.2958555519580841\n",
      "410 0.1434270441532135 0.30346110463142395\n",
      "411 0.14312657713890076 0.30521443486213684\n",
      "412 0.14301985502243042 0.297122985124588\n",
      "413 0.1430015116930008 0.30715835094451904\n",
      "414 0.14283473789691925 0.2987068295478821\n",
      "415 0.14259378612041473 0.2981228828430176\n",
      "416 0.14244578778743744 0.3043725788593292\n",
      "417 0.1423751264810562 0.2953859269618988\n",
      "418 0.1422528624534607 0.3020494878292084\n",
      "419 0.1420266032218933 0.29884031414985657\n",
      "420 0.14185099303722382 0.2969018518924713\n",
      "421 0.14174789190292358 0.30281928181648254\n",
      "422 0.14167451858520508 0.2931511104106903\n",
      "423 0.14155343174934387 0.2978731691837311\n",
      "424 0.14138342440128326 0.29248061776161194\n",
      "425 0.1412084698677063 0.2922849953174591\n",
      "426 0.1410800814628601 0.29494842886924744\n",
      "427 0.14099697768688202 0.2882933020591736\n",
      "428 0.14091230928897858 0.2943113148212433\n",
      "429 0.14080309867858887 0.28774556517601013\n",
      "430 0.14066211879253387 0.29186221957206726\n",
      "431 0.14051048457622528 0.2892635762691498\n",
      "432 0.1403740793466568 0.28885504603385925\n",
      "433 0.14024941623210907 0.29010921716690063\n",
      "434 0.14014030992984772 0.2868027687072754\n",
      "435 0.1400362253189087 0.29089710116386414\n",
      "436 0.1399311125278473 0.2868140637874603\n",
      "437 0.13981950283050537 0.2907378673553467\n",
      "438 0.13970032334327698 0.2861434519290924\n",
      "439 0.13958878815174103 0.2890922725200653\n",
      "440 0.13948668539524078 0.28490886092185974\n",
      "441 0.13937623798847198 0.28838974237442017\n",
      "442 0.1392814666032791 0.2824864983558655\n",
      "443 0.13916103541851044 0.2857694923877716\n",
      "444 0.13908708095550537 0.28070834279060364\n",
      "445 0.13897652924060822 0.28737542033195496\n",
      "446 0.13893342018127441 0.2785510718822479\n",
      "447 0.13890866935253143 0.2901966869831085\n",
      "448 0.13893122971057892 0.27737757563591003\n",
      "449 0.1390971690416336 0.2969680726528168\n",
      "450 0.13953033089637756 0.27283498644828796\n",
      "451 0.14037637412548065 0.3088472783565521\n",
      "452 0.14207078516483307 0.27196305990219116\n",
      "453 0.14469052851200104 0.3332202434539795\n",
      "454 0.14884677529335022 0.2794248163700104\n",
      "455 0.15202300250530243 0.34035298228263855\n",
      "456 0.15271688997745514 0.27040061354637146\n",
      "457 0.14615191519260406 0.28569352626800537\n",
      "458 0.1392345130443573 0.280691534280777\n",
      "459 0.1384590119123459 0.267185777425766\n",
      "460 0.14288665354251862 0.3091984987258911\n",
      "461 0.14468736946582794 0.26753464341163635\n",
      "462 0.14034058153629303 0.2760199010372162\n",
      "463 0.13733430206775665 0.2940671741962433\n",
      "464 0.1394270956516266 0.2685854732990265\n",
      "465 0.1417805701494217 0.2929670810699463\n",
      "466 0.1402662694454193 0.2676890194416046\n",
      "467 0.137290358543396 0.26510968804359436\n",
      "468 0.13763628900051117 0.28822290897369385\n",
      "469 0.1396908462047577 0.26125577092170715\n",
      "470 0.13914844393730164 0.27030929923057556\n",
      "471 0.1371278017759323 0.26821547746658325\n",
      "472 0.1367693394422531 0.26005956530570984\n",
      "473 0.1380663514137268 0.27958154678344727\n",
      "474 0.13824550807476044 0.259046345949173\n",
      "475 0.1368759125471115 0.2621435821056366\n",
      "476 0.13616245985031128 0.27083107829093933\n",
      "477 0.13682766258716583 0.25677821040153503\n",
      "478 0.13740313053131104 0.26965340971946716\n",
      "479 0.13677258789539337 0.25856566429138184\n",
      "480 0.13584043085575104 0.2584627866744995\n",
      "481 0.13573528826236725 0.2687757909297943\n",
      "482 0.1362658143043518 0.25519615411758423\n",
      "483 0.13646821677684784 0.26577842235565186\n",
      "484 0.13596591353416443 0.2563500702381134\n",
      "485 0.1353175789117813 0.2568507492542267\n",
      "486 0.1351393759250641 0.2635013163089752\n",
      "487 0.13542594015598297 0.25275465846061707\n",
      "488 0.1356569230556488 0.262585312128067\n",
      "489 0.13550013303756714 0.25142863392829895\n",
      "490 0.13504692912101746 0.2542419135570526\n",
      "491 0.13465538620948792 0.2536483705043793\n",
      "492 0.134561225771904 0.2488059401512146\n",
      "493 0.13465766608715057 0.25551173090934753\n",
      "494 0.13477489352226257 0.246018648147583\n",
      "495 0.13469943404197693 0.2529502809047699\n",
      "496 0.13452135026454926 0.24460838735103607\n",
      "497 0.13427981734275818 0.24881041049957275\n",
      "498 0.13404397666454315 0.2467304915189743\n",
      "499 0.1338791698217392 0.2460906058549881\n"
     ]
    }
   ],
   "source": [
    "for row in loc:\n",
    "    y_train = SST[1:-15, :, row[1], row[0]]\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_valid = SST[-15:-3, :, row[1], row[0]]\n",
    "    y_valid = torch.from_numpy(y_valid)\n",
    "\n",
    "    for epoch in range(500):  # loop over the dataset multiple times\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        outputs_valid = net(X_valid)\n",
    "        loss_valid = criterion(outputs_valid, y_valid)\n",
    "        print(epoch, loss.item(), loss_valid.item())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测（92°W，0°）今年3月份海温（真实值为27.26599℃）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 26.5089]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = SST[-2].reshape((1, 1, 89, 180))\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "y_fore = net(X_test) + month_mean[2, row[1], row[0]]\n",
    "y_fore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler #only can be imported above pytorch0.2.0\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.pool0 = nn.AvgPool2d((2, 4), padding=1)\n",
    "        self.conv1 = nn.Conv2d(2, 5, 4)\n",
    "        self.pool1 = nn.AvgPool2d(3)\n",
    "        self.conv2 = nn.Conv2d(5, 5, 3)\n",
    "        self.pool2 = nn.AvgPool2d(3)\n",
    "        self.fc1 = nn.Linear(80, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool0(x)\n",
    "        x = self.pool1(F.relu(self.conv1(x)))\n",
    "        x = self.pool2(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 5*4*4)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4  # netCDF4非Python自带包，需要自行下载\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "f = netCDF4.Dataset('D:\\\\sst.mnmean.nc')  # ftp://ftp.cdc.noaa.gov/Datasets/noaa.ersst.v5/sst.mnmean.nc\n",
    "SST = f.variables['sst'][-1203:, :, :].data\n",
    "\n",
    "SST[SST < -2] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_mean = np.zeros((12, 89, 180))\n",
    "month_s = np.arange(0, 1188, 12)\n",
    "for i in range(12):\n",
    "    month_mean[i] = np.average(SST[month_s + i], axis = 0)\n",
    "    SST[month_s + i] -= month_mean[i]\n",
    "for i in range(12):\n",
    "    SST[1188 + i] -= month_mean[i]\n",
    "for i in range(3):\n",
    "    SST[1200 + i] -= month_mean[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST = SST.reshape((1203, 1, 89, 180))\n",
    "\n",
    "X_train = np.zeros((1186, 2, 89, 180), dtype=np.float32)\n",
    "X_train[:, 0] = SST[:-17, 0]\n",
    "X_train[:, 1] = SST[1:-16, 0]\n",
    "X_train = torch.from_numpy(X_train)\n",
    "\n",
    "X_valid = np.zeros((12, 2, 89, 180), dtype=np.float32)\n",
    "X_valid[:, 0] = SST[-17:-5, 0]\n",
    "X_valid[:, 1] = SST[-16:-4, 0]\n",
    "X_valid = torch.from_numpy(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = ((134, 44), \\\n",
    "       (94, 44), \\\n",
    "       (57, 37), \\\n",
    "       (39, 44), \\\n",
    "       (21, 36), \\\n",
    "       (9, 16), \\\n",
    "       (172, 59), \\\n",
    "       (158, 28), \\\n",
    "       (65, 64), \\\n",
    "       (0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0924222469329834 0.7783412933349609\n",
      "1 1.0704162120819092 0.7477688789367676\n",
      "2 1.0477246046066284 0.7139258980751038\n",
      "3 1.017866849899292 0.689863920211792\n",
      "4 0.9753577709197998 0.7009252905845642\n",
      "5 0.9219707250595093 0.7884209156036377\n",
      "6 0.8792669177055359 0.9841131567955017\n",
      "7 0.8626843690872192 1.0334067344665527\n",
      "8 0.8389838337898254 0.8623300194740295\n",
      "9 0.7954409718513489 0.656138002872467\n",
      "10 0.7594743371009827 0.5161804556846619\n",
      "11 0.7386359572410583 0.43809643387794495\n",
      "12 0.7192434072494507 0.39140284061431885\n",
      "13 0.6935243606567383 0.3567187786102295\n",
      "14 0.6629032492637634 0.3238140642642975\n",
      "15 0.6298184394836426 0.28833717107772827\n",
      "16 0.5976099371910095 0.2548743784427643\n",
      "17 0.5731301307678223 0.2385539561510086\n",
      "18 0.5558366179466248 0.27531805634498596\n",
      "19 0.5374846458435059 0.3896186649799347\n",
      "20 0.5245131850242615 0.4896676540374756\n",
      "21 0.5149450898170471 0.49002495408058167\n",
      "22 0.5047157406806946 0.4387580156326294\n",
      "23 0.4967308044433594 0.39900103211402893\n",
      "24 0.48543277382850647 0.3703765869140625\n",
      "25 0.4717923402786255 0.3218381702899933\n",
      "26 0.4596249759197235 0.25130513310432434\n",
      "27 0.44799959659576416 0.20110167562961578\n",
      "28 0.4380478858947754 0.2030043601989746\n",
      "29 0.42641711235046387 0.21643869578838348\n",
      "30 0.41661232709884644 0.21521921455860138\n",
      "31 0.40708696842193604 0.21974481642246246\n",
      "32 0.4000978469848633 0.2297559529542923\n",
      "33 0.392184317111969 0.22142820060253143\n",
      "34 0.38545623421669006 0.2094496339559555\n",
      "35 0.3819887936115265 0.22042697668075562\n",
      "36 0.3770153522491455 0.2244284749031067\n",
      "37 0.37131890654563904 0.22595198452472687\n",
      "38 0.36717382073402405 0.24765627086162567\n",
      "39 0.36187222599983215 0.2311825007200241\n",
      "40 0.35524114966392517 0.22613327205181122\n",
      "41 0.3507286310195923 0.2399800568819046\n",
      "42 0.3471028804779053 0.2249385118484497\n",
      "43 0.34248536825180054 0.22800488770008087\n",
      "44 0.33868107199668884 0.2412165403366089\n",
      "45 0.33572113513946533 0.22354058921337128\n",
      "46 0.33168697357177734 0.2252669781446457\n",
      "47 0.3274123966693878 0.2312137484550476\n",
      "48 0.32394230365753174 0.20950984954833984\n",
      "49 0.3202035129070282 0.21526449918746948\n",
      "50 0.3162674307823181 0.21632333099842072\n",
      "51 0.3129269778728485 0.19670706987380981\n",
      "52 0.30966973304748535 0.19538579881191254\n",
      "53 0.3062196969985962 0.182473823428154\n",
      "54 0.30303797125816345 0.16794878244400024\n",
      "55 0.30011633038520813 0.16786153614521027\n",
      "56 0.29692554473876953 0.15889476239681244\n",
      "57 0.2934778332710266 0.15528647601604462\n",
      "58 0.29014116525650024 0.15282216668128967\n",
      "59 0.2870916724205017 0.1406315714120865\n",
      "60 0.28422531485557556 0.14203007519245148\n",
      "61 0.2814764976501465 0.1309279203414917\n",
      "62 0.27861154079437256 0.13036854565143585\n",
      "63 0.2757045328617096 0.12435082346200943\n",
      "64 0.272830069065094 0.11946318298578262\n",
      "65 0.2699921429157257 0.11601895093917847\n",
      "66 0.2672661542892456 0.11223863810300827\n",
      "67 0.264822781085968 0.11100342869758606\n",
      "68 0.2631849944591522 0.11467669159173965\n",
      "69 0.2641116976737976 0.12600821256637573\n",
      "70 0.27100273966789246 0.11675187945365906\n",
      "71 0.2676233947277069 0.10289756208658218\n",
      "72 0.2542327642440796 0.10594471544027328\n",
      "73 0.2559542655944824 0.10043427348136902\n",
      "74 0.25670039653778076 0.09168293327093124\n",
      "75 0.24787583947181702 0.09765505790710449\n",
      "76 0.2509387731552124 0.09213561564683914\n",
      "77 0.24713382124900818 0.08910100907087326\n",
      "78 0.24371284246444702 0.09278292208909988\n",
      "79 0.24545925855636597 0.08652865886688232\n",
      "80 0.23984012007713318 0.08941806107759476\n",
      "81 0.2405998706817627 0.08777552098035812\n",
      "82 0.23901279270648956 0.0844927504658699\n",
      "83 0.23510490357875824 0.08930903673171997\n",
      "84 0.23650497198104858 0.08364146947860718\n",
      "85 0.23363439738750458 0.08142147213220596\n",
      "86 0.2311049848794937 0.08680769056081772\n",
      "87 0.2320493906736374 0.07837755233049393\n",
      "88 0.22924591600894928 0.0768733024597168\n",
      "89 0.2269643247127533 0.08146292716264725\n",
      "90 0.22754572331905365 0.07506918162107468\n",
      "91 0.2256142646074295 0.07369013875722885\n",
      "92 0.22306376695632935 0.07515468448400497\n",
      "93 0.22318439185619354 0.07330527901649475\n",
      "94 0.2225615382194519 0.07164060324430466\n",
      "95 0.22003227472305298 0.0708935409784317\n",
      "96 0.21878010034561157 0.07098504155874252\n",
      "97 0.21881917119026184 0.07258681207895279\n",
      "98 0.21783287823200226 0.06914638727903366\n",
      "99 0.2158776968717575 0.06893796473741531\n",
      "100 0.21453402936458588 0.07104820758104324\n",
      "101 0.2141820788383484 0.06839792430400848\n",
      "102 0.21389047801494598 0.07202479243278503\n",
      "103 0.21290437877178192 0.06778018921613693\n",
      "104 0.2114800661802292 0.06872892379760742\n",
      "105 0.21009495854377747 0.06765413284301758\n",
      "106 0.20913535356521606 0.06628478318452835\n",
      "107 0.20858976244926453 0.06945329904556274\n",
      "108 0.20825758576393127 0.06553430110216141\n",
      "109 0.2080060839653015 0.0711241066455841\n",
      "110 0.2076461911201477 0.06533221155405045\n",
      "111 0.2071695774793625 0.0708104819059372\n",
      "112 0.20640230178833008 0.06530917435884476\n",
      "113 0.20546841621398926 0.06898027658462524\n",
      "114 0.20433126389980316 0.06551546603441238\n",
      "115 0.20320704579353333 0.06717663258314133\n",
      "116 0.20219649374485016 0.06602708995342255\n",
      "117 0.20138420164585114 0.06618645042181015\n",
      "118 0.20077043771743774 0.0671086311340332\n",
      "119 0.20031186938285828 0.06611847877502441\n",
      "120 0.1999901384115219 0.06888889521360397\n",
      "121 0.19986087083816528 0.06708162277936935\n",
      "122 0.2001132071018219 0.07272705435752869\n",
      "123 0.20098696649074554 0.07076962292194366\n",
      "124 0.20301775634288788 0.08047204464673996\n",
      "125 0.20551805198192596 0.07545854896306992\n",
      "126 0.20698373019695282 0.07711789011955261\n",
      "127 0.20244044065475464 0.06710421293973923\n",
      "128 0.1966468244791031 0.06697510927915573\n",
      "129 0.19624124467372894 0.07461060583591461\n",
      "130 0.1996067613363266 0.0704093649983406\n",
      "131 0.19959740340709686 0.06992394477128983\n",
      "132 0.19530834257602692 0.06906196475028992\n",
      "133 0.19416934251785278 0.06991278380155563\n",
      "134 0.19653499126434326 0.0750870630145073\n",
      "135 0.19638174772262573 0.06889849156141281\n",
      "136 0.193476602435112 0.06933540105819702\n",
      "137 0.19232617318630219 0.07503295689821243\n",
      "138 0.19374440610408783 0.07098875939846039\n",
      "139 0.19424739480018616 0.07492019981145859\n",
      "140 0.19233821332454681 0.07167372107505798\n",
      "141 0.1906864494085312 0.07188301533460617\n",
      "142 0.1909608393907547 0.07710277289152145\n",
      "143 0.1918407827615738 0.0739903375506401\n",
      "144 0.19163371622562408 0.07681948691606522\n",
      "145 0.1902005821466446 0.07492619752883911\n",
      "146 0.1889488399028778 0.0756964161992073\n",
      "147 0.18871481716632843 0.079464390873909\n",
      "148 0.18913719058036804 0.07812023907899857\n",
      "149 0.189384326338768 0.08193010836839676\n",
      "150 0.188907191157341 0.07982803136110306\n",
      "151 0.18791773915290833 0.08132438361644745\n",
      "152 0.1869620978832245 0.08156798034906387\n",
      "153 0.186440572142601 0.0821145623922348\n",
      "154 0.18635641038417816 0.08437559008598328\n",
      "155 0.18647333979606628 0.08428037166595459\n",
      "156 0.18658185005187988 0.08704953640699387\n",
      "157 0.18650156259536743 0.0863732099533081\n",
      "158 0.18622185289859772 0.0888400599360466\n",
      "159 0.18569673597812653 0.0880366861820221\n",
      "160 0.18506421148777008 0.09033051878213882\n",
      "161 0.1844029277563095 0.09002948552370071\n",
      "162 0.18382905423641205 0.09188243746757507\n",
      "163 0.1833549439907074 0.09216943383216858\n",
      "164 0.18297536671161652 0.09294140338897705\n",
      "165 0.18266645073890686 0.09381493926048279\n",
      "166 0.18240851163864136 0.09359369426965714\n",
      "167 0.18219855427742004 0.09543826431035995\n",
      "168 0.18206001818180084 0.09473072737455368\n",
      "169 0.18206019699573517 0.09828854352235794\n",
      "170 0.1823655515909195 0.09690433740615845\n",
      "171 0.1834007054567337 0.104964978992939\n",
      "172 0.1859431266784668 0.10403713583946228\n",
      "173 0.19130629301071167 0.11868899315595627\n",
      "174 0.19780829548835754 0.11261580139398575\n",
      "175 0.19929219782352448 0.10194715112447739\n",
      "176 0.18695233762264252 0.09330286830663681\n",
      "177 0.17989785969257355 0.10243314504623413\n",
      "178 0.1872663050889969 0.09909559041261673\n",
      "179 0.18784023821353912 0.0953427329659462\n",
      "180 0.18001891672611237 0.099854476749897\n",
      "181 0.1820480078458786 0.10106856375932693\n",
      "182 0.18515494465827942 0.09937392920255661\n",
      "183 0.1798703670501709 0.10027063637971878\n",
      "184 0.17985732853412628 0.10586138814687729\n",
      "185 0.18296931684017181 0.10214465856552124\n",
      "186 0.17960500717163086 0.10283804684877396\n",
      "187 0.17809483408927917 0.10990998148918152\n",
      "188 0.18087820708751678 0.10764067620038986\n",
      "189 0.17967188358306885 0.10738840699195862\n",
      "190 0.17708364129066467 0.11010947823524475\n",
      "191 0.17854540050029755 0.1108461543917656\n",
      "192 0.1795681118965149 0.10912834852933884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 0.17737534642219543 0.10822869092226028\n",
      "194 0.17647625505924225 0.11043223738670349\n",
      "195 0.17786617577075958 0.10831903666257858\n",
      "196 0.1777893453836441 0.10807955265045166\n",
      "197 0.17614613473415375 0.10726223140954971\n",
      "198 0.17587417364120483 0.1052391454577446\n",
      "199 0.17678102850914001 0.10809841006994247\n",
      "200 0.17647632956504822 0.10445096343755722\n",
      "201 0.17529596388339996 0.10432963818311691\n",
      "202 0.17525231838226318 0.1078687533736229\n",
      "203 0.17580030858516693 0.10448169708251953\n",
      "204 0.17538657784461975 0.10577940940856934\n",
      "205 0.17459642887115479 0.10627207159996033\n",
      "206 0.17458784580230713 0.1050928607583046\n",
      "207 0.174891397356987 0.10685473680496216\n",
      "208 0.1745796501636505 0.10535714775323868\n",
      "209 0.17398780584335327 0.10542616993188858\n",
      "210 0.17385916411876678 0.1068679466843605\n",
      "211 0.17403532564640045 0.10561370849609375\n",
      "212 0.17391647398471832 0.10663079470396042\n",
      "213 0.1734778881072998 0.10603330284357071\n",
      "214 0.17316702008247375 0.10555749386548996\n",
      "215 0.17317035794258118 0.10679062455892563\n",
      "216 0.17320270836353302 0.10505887866020203\n",
      "217 0.17300352454185486 0.10579229146242142\n",
      "218 0.172674760222435 0.10543220490217209\n",
      "219 0.1724478006362915 0.10543348640203476\n",
      "220 0.17238658666610718 0.10679512470960617\n",
      "221 0.1723625659942627 0.10623029619455338\n",
      "222 0.17223051190376282 0.10721767693758011\n",
      "223 0.17199374735355377 0.10663876682519913\n",
      "224 0.1717590093612671 0.10669777542352676\n",
      "225 0.17160187661647797 0.10702820867300034\n",
      "226 0.17151564359664917 0.10674635320901871\n",
      "227 0.17143988609313965 0.10738825052976608\n",
      "228 0.17132693529129028 0.1068333312869072\n",
      "229 0.17116627097129822 0.10716116428375244\n",
      "230 0.17097872495651245 0.10669207572937012\n",
      "231 0.1707969754934311 0.10671738535165787\n",
      "232 0.17064169049263 0.10686608403921127\n",
      "233 0.17051266133785248 0.1069052517414093\n",
      "234 0.17039993405342102 0.1076599732041359\n",
      "235 0.17029231786727905 0.10758741945028305\n",
      "236 0.17018470168113708 0.10839945822954178\n",
      "237 0.17008157074451447 0.10767730325460434\n",
      "238 0.16999146342277527 0.10831556469202042\n",
      "239 0.16991674900054932 0.10725080966949463\n",
      "240 0.16987106204032898 0.10838744789361954\n",
      "241 0.16988369822502136 0.10724148899316788\n",
      "242 0.1699828952550888 0.10921311378479004\n",
      "243 0.1702333241701126 0.10757037997245789\n",
      "244 0.17074055969715118 0.11099052429199219\n",
      "245 0.1716540902853012 0.10875237733125687\n",
      "246 0.17303134500980377 0.11476290225982666\n",
      "247 0.1749527007341385 0.11021996289491653\n",
      "248 0.17645494639873505 0.11644297093153\n",
      "249 0.17653150856494904 0.10586639493703842\n",
      "250 0.17357459664344788 0.1068982258439064\n",
      "251 0.16972993314266205 0.10377920418977737\n",
      "252 0.16822361946105957 0.10393478721380234\n",
      "253 0.16982990503311157 0.11123769730329514\n",
      "254 0.1717773824930191 0.10782714933156967\n",
      "255 0.1713273674249649 0.10908900946378708\n",
      "256 0.16907458007335663 0.10727477818727493\n",
      "257 0.16758166253566742 0.10806030035018921\n",
      "258 0.16823504865169525 0.11013873666524887\n",
      "259 0.16969479620456696 0.11001060158014297\n",
      "260 0.16997264325618744 0.11043577641248703\n",
      "261 0.16879181563854218 0.10804959386587143\n",
      "262 0.16731373965740204 0.10779815912246704\n",
      "263 0.1669035702943802 0.10932225733995438\n",
      "264 0.16755615174770355 0.10708221793174744\n",
      "265 0.16823169589042664 0.11016661673784256\n",
      "266 0.16810348629951477 0.10692181438207626\n",
      "267 0.1672244369983673 0.10839959979057312\n",
      "268 0.16640247404575348 0.10826488584280014\n",
      "269 0.1661723405122757 0.10769054293632507\n",
      "270 0.16647173464298248 0.10983224958181381\n",
      "271 0.1668429970741272 0.10815488547086716\n",
      "272 0.16687393188476562 0.10978147387504578\n",
      "273 0.166514590382576 0.10854580253362656\n",
      "274 0.16595999896526337 0.10937214642763138\n",
      "275 0.16551737487316132 0.10952609777450562\n",
      "276 0.16533604264259338 0.10953675955533981\n",
      "277 0.16539546847343445 0.11083060503005981\n",
      "278 0.16555514931678772 0.11008156090974808\n",
      "279 0.16565176844596863 0.11151639372110367\n",
      "280 0.16562199592590332 0.11028790473937988\n",
      "281 0.1654413342475891 0.1111484169960022\n",
      "282 0.16516636312007904 0.11007291078567505\n",
      "283 0.16485638916492462 0.11057118326425552\n",
      "284 0.1645878553390503 0.11038509011268616\n",
      "285 0.1643781214952469 0.1109243854880333\n",
      "286 0.164219930768013 0.11132308095693588\n",
      "287 0.16410787403583527 0.11141040921211243\n",
      "288 0.16403335332870483 0.111707903444767\n",
      "289 0.16398079693317413 0.11130432039499283\n",
      "290 0.163955956697464 0.11159073561429977\n",
      "291 0.1639791578054428 0.11122119426727295\n",
      "292 0.16406798362731934 0.11219724267721176\n",
      "293 0.1642768234014511 0.11196702718734741\n",
      "294 0.1646667867898941 0.11372756212949753\n",
      "295 0.16534008085727692 0.11320114880800247\n",
      "296 0.16638602316379547 0.11614138633012772\n",
      "297 0.1679340898990631 0.1155528649687767\n",
      "298 0.16957128047943115 0.11887139827013016\n",
      "299 0.170810267329216 0.1152147427201271\n",
      "300 0.16998940706253052 0.11460814625024796\n",
      "301 0.1670222133398056 0.10952624678611755\n",
      "302 0.16360770165920258 0.10932224243879318\n",
      "303 0.16264301538467407 0.11205220967531204\n",
      "304 0.1643000692129135 0.11448008567094803\n",
      "305 0.16601327061653137 0.11468368768692017\n",
      "306 0.1656569093465805 0.11379150301218033\n",
      "307 0.16358010470867157 0.1124294176697731\n",
      "308 0.16208399832248688 0.11298833042383194\n",
      "309 0.1624481976032257 0.11483938246965408\n",
      "310 0.16372784972190857 0.11562681198120117\n",
      "311 0.16424202919006348 0.11496761441230774\n",
      "312 0.16341468691825867 0.11430689692497253\n",
      "313 0.1621014028787613 0.11352009326219559\n",
      "314 0.16144052147865295 0.11361601203680038\n",
      "315 0.16178584098815918 0.11473339051008224\n",
      "316 0.16248014569282532 0.1143733561038971\n",
      "317 0.1626504510641098 0.11471732705831528\n",
      "318 0.16209810972213745 0.11365467309951782\n",
      "319 0.16128754615783691 0.11366871744394302\n",
      "320 0.16083846986293793 0.11401178687810898\n",
      "321 0.16093111038208008 0.11438222974538803\n",
      "322 0.16127878427505493 0.11516506224870682\n",
      "323 0.16146615147590637 0.11591584235429764\n",
      "324 0.16129538416862488 0.11605367064476013\n",
      "325 0.1608853042125702 0.11638285964727402\n",
      "326 0.1604434698820114 0.11611095070838928\n",
      "327 0.16015195846557617 0.11598245054483414\n",
      "328 0.16007418930530548 0.11646620184183121\n",
      "329 0.16016168892383575 0.11602955311536789\n",
      "330 0.1602952778339386 0.1173485517501831\n",
      "331 0.1603860706090927 0.11691620200872421\n",
      "332 0.1603861302137375 0.11807625740766525\n",
      "333 0.16027356684207916 0.11751449108123779\n",
      "334 0.16007278859615326 0.11820006370544434\n",
      "335 0.1598200500011444 0.11758030205965042\n",
      "336 0.15957367420196533 0.11833053827285767\n",
      "337 0.15934517979621887 0.11822265386581421\n",
      "338 0.15915827453136444 0.11880755424499512\n",
      "339 0.15901415050029755 0.11900561302900314\n",
      "340 0.15890368819236755 0.11916434019804001\n",
      "341 0.1588209867477417 0.11955094337463379\n",
      "342 0.15876422822475433 0.11931031942367554\n",
      "343 0.15873338282108307 0.11993642896413803\n",
      "344 0.15874448418617249 0.1194162592291832\n",
      "345 0.1588195413351059 0.12038253992795944\n",
      "346 0.15901637077331543 0.11960501223802567\n",
      "347 0.1594361960887909 0.12181568145751953\n",
      "348 0.16023258864879608 0.12108691781759262\n",
      "349 0.16169507801532745 0.12583307921886444\n",
      "350 0.16401121020317078 0.12530873715877533\n",
      "351 0.1671406477689743 0.13008460402488708\n",
      "352 0.16944561898708344 0.12615220248699188\n",
      "353 0.16872964799404144 0.12160090357065201\n",
      "354 0.16348573565483093 0.11560583114624023\n",
      "355 0.15843282639980316 0.11575672030448914\n",
      "356 0.15858851373195648 0.12157195061445236\n",
      "357 0.16205866634845734 0.12208790332078934\n",
      "358 0.16309334337711334 0.1240759789943695\n",
      "359 0.15998655557632446 0.12168154120445251\n",
      "360 0.15738491714000702 0.12283691018819809\n",
      "361 0.15839946269989014 0.1292402446269989\n",
      "362 0.16054634749889374 0.12386665493249893\n",
      "363 0.1603642851114273 0.12582680583000183\n",
      "364 0.15809229016304016 0.12296966463327408\n",
      "365 0.1568165421485901 0.12257736921310425\n",
      "366 0.1576918363571167 0.12612242996692657\n",
      "367 0.15877218544483185 0.12259310483932495\n",
      "368 0.15827110409736633 0.12265682965517044\n",
      "369 0.15684497356414795 0.12193091958761215\n",
      "370 0.15638694167137146 0.1222255602478981\n",
      "371 0.15708300471305847 0.12500078976154327\n",
      "372 0.15757612884044647 0.12335322052240372\n",
      "373 0.15708096325397491 0.12470802664756775\n",
      "374 0.15618978440761566 0.12468540668487549\n",
      "375 0.15589451789855957 0.12434199452400208\n",
      "376 0.1562688797712326 0.1273287534713745\n",
      "377 0.1566079556941986 0.12512534856796265\n",
      "378 0.15643641352653503 0.127420574426651\n",
      "379 0.15587937831878662 0.126376211643219\n",
      "380 0.15543977916240692 0.1265217810869217\n",
      "381 0.1554015576839447 0.1280815750360489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382 0.15562200546264648 0.1267538219690323\n",
      "383 0.15575744211673737 0.12887120246887207\n",
      "384 0.15559297800064087 0.12736012041568756\n",
      "385 0.15524767339229584 0.12829825282096863\n",
      "386 0.15493090450763702 0.1281834840774536\n",
      "387 0.15479522943496704 0.1277867704629898\n",
      "388 0.15483234822750092 0.12933449447155\n",
      "389 0.15490949153900146 0.12841914594173431\n",
      "390 0.15490420162677765 0.13042187690734863\n",
      "391 0.154795303940773 0.12926225364208221\n",
      "392 0.1545923501253128 0.1303350329399109\n",
      "393 0.15436862409114838 0.12970729172229767\n",
      "394 0.1541934460401535 0.1298225373029709\n",
      "395 0.15409450232982635 0.13034918904304504\n",
      "396 0.15405558049678802 0.13000120222568512\n",
      "397 0.15404854714870453 0.1312214881181717\n",
      "398 0.15404565632343292 0.13034051656723022\n",
      "399 0.15402184426784515 0.1318170428276062\n",
      "400 0.15396752953529358 0.13082094490528107\n",
      "401 0.15389753878116608 0.13268032670021057\n",
      "402 0.15380370616912842 0.1316050887107849\n",
      "403 0.1537168025970459 0.13347980380058289\n",
      "404 0.15363629162311554 0.13218368589878082\n",
      "405 0.15356752276420593 0.13394203782081604\n",
      "406 0.15351951122283936 0.13220027089118958\n",
      "407 0.15351103246212006 0.13409963250160217\n",
      "408 0.15354184806346893 0.13204126060009003\n",
      "409 0.15365667641162872 0.13477551937103271\n",
      "410 0.15387366712093353 0.1324726790189743\n",
      "411 0.15427444875240326 0.13651365041732788\n",
      "412 0.1549188643693924 0.13363809883594513\n",
      "413 0.15593206882476807 0.1394190937280655\n",
      "414 0.15723150968551636 0.13547877967357635\n",
      "415 0.15873323380947113 0.14125388860702515\n",
      "416 0.15961460769176483 0.1358901858329773\n",
      "417 0.15923790633678436 0.13647142052650452\n",
      "418 0.1568278968334198 0.1317262053489685\n",
      "419 0.15385740995407104 0.13107337057590485\n",
      "420 0.15225522220134735 0.13308028876781464\n",
      "421 0.1528346836566925 0.1339670866727829\n",
      "422 0.15447397530078888 0.14068928360939026\n",
      "423 0.15538635849952698 0.13621090352535248\n",
      "424 0.15488971769809723 0.1398020088672638\n",
      "425 0.15328817069530487 0.13559465110301971\n",
      "426 0.1519448459148407 0.1349496692419052\n",
      "427 0.15168754756450653 0.13631106913089752\n",
      "428 0.15234597027301788 0.1340108960866928\n",
      "429 0.15320000052452087 0.136886328458786\n",
      "430 0.1536097377538681 0.13401897251605988\n",
      "431 0.1532871574163437 0.13487790524959564\n",
      "432 0.15238694846630096 0.13299579918384552\n",
      "433 0.15149542689323425 0.13332073390483856\n",
      "434 0.15108276903629303 0.1343245655298233\n",
      "435 0.1512061357498169 0.1342438906431198\n",
      "436 0.15162527561187744 0.13742507994174957\n",
      "437 0.1520576775074005 0.1355670988559723\n",
      "438 0.15225055813789368 0.138246551156044\n",
      "439 0.15211768448352814 0.13540293276309967\n",
      "440 0.15174084901809692 0.13687516748905182\n",
      "441 0.15125946700572968 0.13508722186088562\n",
      "442 0.150792196393013 0.1360960453748703\n",
      "443 0.1504591554403305 0.1363356113433838\n",
      "444 0.1503005027770996 0.1368323266506195\n",
      "445 0.15029080212116241 0.13849897682666779\n",
      "446 0.15037967264652252 0.13760529458522797\n",
      "447 0.1505095511674881 0.140043705701828\n",
      "448 0.150633305311203 0.1379878669977188\n",
      "449 0.15073296427726746 0.1411031186580658\n",
      "450 0.1507934331893921 0.13867929577827454\n",
      "451 0.15083223581314087 0.14188776910305023\n",
      "452 0.15083543956279755 0.13940538465976715\n",
      "453 0.15083160996437073 0.14319965243339539\n",
      "454 0.15082798898220062 0.14037315547466278\n",
      "455 0.15084265172481537 0.14479656517505646\n",
      "456 0.1508508175611496 0.1413201093673706\n",
      "457 0.15085898339748383 0.14563339948654175\n",
      "458 0.1508660912513733 0.14154191315174103\n",
      "459 0.1508740335702896 0.14601044356822968\n",
      "460 0.15087579190731049 0.14164307713508606\n",
      "461 0.15085965394973755 0.14642100036144257\n",
      "462 0.15079642832279205 0.14215795695781708\n",
      "463 0.15068532526493073 0.146654412150383\n",
      "464 0.1505180448293686 0.14245858788490295\n",
      "465 0.1503019779920578 0.14674724638462067\n",
      "466 0.15009739995002747 0.14265893399715424\n",
      "467 0.14986665546894073 0.14681608974933624\n",
      "468 0.14966079592704773 0.1431518942117691\n",
      "469 0.14945459365844727 0.14676320552825928\n",
      "470 0.14926892518997192 0.14339081943035126\n",
      "471 0.14907516539096832 0.14646852016448975\n",
      "472 0.1489027589559555 0.14355693757534027\n",
      "473 0.1487380564212799 0.1463346630334854\n",
      "474 0.14860546588897705 0.14371085166931152\n",
      "475 0.14850255846977234 0.1463298797607422\n",
      "476 0.1484433114528656 0.1435505896806717\n",
      "477 0.1484328955411911 0.14658677577972412\n",
      "478 0.14848200976848602 0.1433507353067398\n",
      "479 0.14859874546527863 0.1473696380853653\n",
      "480 0.14882367849349976 0.14350372552871704\n",
      "481 0.14925841987133026 0.14935943484306335\n",
      "482 0.14999663829803467 0.14414380490779877\n",
      "483 0.15118753910064697 0.1530354917049408\n",
      "484 0.15280282497406006 0.1460263580083847\n",
      "485 0.15489935874938965 0.15680985152721405\n",
      "486 0.15647752583026886 0.14691700041294098\n",
      "487 0.1565997153520584 0.1521180421113968\n",
      "488 0.15390776097774506 0.14204826951026917\n",
      "489 0.1497729867696762 0.14205248653888702\n",
      "490 0.14715714752674103 0.14399534463882446\n",
      "491 0.14776194095611572 0.14386199414730072\n",
      "492 0.15010632574558258 0.15215153992176056\n",
      "493 0.15140803158283234 0.14585167169570923\n",
      "494 0.15049022436141968 0.14933845400810242\n",
      "495 0.14814625680446625 0.14477448165416718\n",
      "496 0.1465679109096527 0.14365209639072418\n",
      "497 0.14680719375610352 0.1474066525697708\n",
      "498 0.14811305701732635 0.14321763813495636\n",
      "499 0.14914001524448395 0.14752419292926788\n"
     ]
    }
   ],
   "source": [
    "for row in loc:\n",
    "    y_train = SST[2:-15, :, row[1], row[0]]\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_valid = SST[-15:-3, :, row[1], row[0]]\n",
    "    y_valid = torch.from_numpy(y_valid)\n",
    "    \n",
    "    net = Net()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(500):  # loop over the dataset multiple times\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        outputs_valid = net(X_valid)\n",
    "        loss_valid = criterion(outputs_valid, y_valid)\n",
    "        print(epoch, loss.item(), loss_valid.item())\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight\n",
      "tensor([[[[ 0.1510, -0.1077, -0.0834,  0.0309],\n",
      "          [-0.0083, -0.0001, -0.1092,  0.1961],\n",
      "          [-0.1258, -0.0724,  0.0615,  0.2049],\n",
      "          [-0.2861, -0.2778, -0.0905,  0.0435]],\n",
      "\n",
      "         [[ 0.2954,  0.3766,  0.2090,  0.3081],\n",
      "          [ 0.2486,  0.3672,  0.3810,  0.5710],\n",
      "          [-0.2887,  0.1020,  0.2948,  0.5198],\n",
      "          [-0.3312,  0.2737,  0.4264,  0.2467]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4285,  0.3927,  0.4240,  0.3062],\n",
      "          [ 0.2997,  0.3778,  0.2909,  0.1247],\n",
      "          [ 0.1028, -0.0430, -0.0430, -0.0094],\n",
      "          [ 0.1991, -0.1548, -0.1521, -0.2047]],\n",
      "\n",
      "         [[-0.3585, -0.3561,  0.0138, -0.1086],\n",
      "          [ 0.0507, -0.1134, -0.1795,  0.0252],\n",
      "          [ 0.1738,  0.0330, -0.1451, -0.1173],\n",
      "          [ 0.1612, -0.1298, -0.3141, -0.5173]]],\n",
      "\n",
      "\n",
      "        [[[-0.1371, -0.0787, -0.1715, -0.0309],\n",
      "          [ 0.0039, -0.1633,  0.0560, -0.0061],\n",
      "          [-0.1149, -0.2434, -0.0266,  0.1660],\n",
      "          [-0.3392, -0.1659,  0.0258,  0.3732]],\n",
      "\n",
      "         [[ 0.1501,  0.3417,  0.4167,  0.4555],\n",
      "          [-0.0870,  0.1352,  0.2660,  0.2673],\n",
      "          [-0.3944, -0.1415,  0.3848,  0.5142],\n",
      "          [-0.4450,  0.0302,  0.4840,  0.4777]]],\n",
      "\n",
      "\n",
      "        [[[-0.1583, -0.1563, -0.3127, -0.2778],\n",
      "          [ 0.2006,  0.0502, -0.3626, -0.1617],\n",
      "          [ 0.1844,  0.2762, -0.0053, -0.0764],\n",
      "          [-0.1449,  0.0084,  0.2963,  0.0297]],\n",
      "\n",
      "         [[ 0.4629,  0.6646,  0.3475,  0.6647],\n",
      "          [ 0.8370,  0.7017,  0.1736, -0.0461],\n",
      "          [ 0.0439,  0.0211, -0.3282, -0.1299],\n",
      "          [-0.9520, -0.3390, -0.3739, -0.1620]]],\n",
      "\n",
      "\n",
      "        [[[-0.0559,  0.2675,  0.3751, -0.2588],\n",
      "          [-0.3625, -0.1360, -0.3303, -0.6004],\n",
      "          [-0.0493,  0.4136,  0.0235, -0.5815],\n",
      "          [-0.2132,  0.2809,  0.1576, -0.4575]],\n",
      "\n",
      "         [[-0.5167, -0.3597, -0.2198, -0.2539],\n",
      "          [-0.5517, -0.2284, -0.3297,  0.0388],\n",
      "          [ 0.0135,  0.1638, -0.1138, -0.3039],\n",
      "          [ 0.3454,  0.4423, -0.2709, -0.7258]]]])\n",
      "conv1.bias\n",
      "tensor([-0.2446,  0.4220,  0.2814,  0.7510,  0.5875])\n",
      "conv2.weight\n",
      "tensor([[[[-0.2499, -0.4626,  0.3272],\n",
      "          [ 0.1160, -0.3250, -0.0463],\n",
      "          [ 0.2020,  0.3089,  0.1159]],\n",
      "\n",
      "         [[ 0.1973, -0.1197, -0.0812],\n",
      "          [ 0.4484,  0.0348,  0.3189],\n",
      "          [-0.2392, -0.0594, -0.0506]],\n",
      "\n",
      "         [[-0.3009, -0.0844,  0.0984],\n",
      "          [-0.1969, -0.0252, -0.2745],\n",
      "          [-0.1823,  0.6401, -0.0894]],\n",
      "\n",
      "         [[ 0.6051,  0.3174, -0.2522],\n",
      "          [-0.2121, -0.7371,  0.6051],\n",
      "          [ 0.5788,  0.3637,  0.1411]],\n",
      "\n",
      "         [[-0.1888, -0.0917,  0.1650],\n",
      "          [ 0.7506,  0.0510,  0.1944],\n",
      "          [ 0.2760, -0.2248, -0.0407]]],\n",
      "\n",
      "\n",
      "        [[[-0.1816, -0.2668, -0.1354],\n",
      "          [-0.0841,  0.9680,  0.0282],\n",
      "          [-0.2288, -0.1297, -0.5382]],\n",
      "\n",
      "         [[ 0.2952, -0.4001,  0.0430],\n",
      "          [ 0.1763,  0.1325,  0.0887],\n",
      "          [ 0.0851,  0.2326, -0.3583]],\n",
      "\n",
      "         [[-0.5769, -0.3497, -0.4607],\n",
      "          [ 0.1989,  0.6201,  0.2780],\n",
      "          [ 0.0215, -0.0620,  0.0642]],\n",
      "\n",
      "         [[-0.2006, -0.3358, -0.0192],\n",
      "          [ 0.9779,  0.5627, -0.0155],\n",
      "          [-0.5387,  0.0809,  0.2561]],\n",
      "\n",
      "         [[-0.1303,  0.3592, -0.4686],\n",
      "          [ 0.1834,  0.2428,  0.3579],\n",
      "          [ 0.2068,  0.0642, -0.1764]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0211, -0.0927, -0.1489],\n",
      "          [ 0.0871,  0.0107,  0.2270],\n",
      "          [-0.5510,  0.0397,  0.0443]],\n",
      "\n",
      "         [[-0.3447, -0.2502, -0.1236],\n",
      "          [-0.0493, -0.0453,  0.1023],\n",
      "          [ 0.0759,  0.1012,  0.2009]],\n",
      "\n",
      "         [[-0.0059, -0.0185,  0.0804],\n",
      "          [ 0.1105,  0.0602,  0.3627],\n",
      "          [-0.4522, -0.0874,  0.4797]],\n",
      "\n",
      "         [[-0.1336,  0.0174, -0.0995],\n",
      "          [-0.0195, -0.1921, -0.0243],\n",
      "          [-0.1147,  0.0646,  0.1708]],\n",
      "\n",
      "         [[-0.1249, -0.2340,  0.3405],\n",
      "          [-0.3876, -0.0232,  0.1827],\n",
      "          [ 0.0662,  0.0554, -0.0251]]],\n",
      "\n",
      "\n",
      "        [[[-0.1229,  0.0901,  0.0245],\n",
      "          [ 0.0667,  0.1387,  0.2496],\n",
      "          [ 0.0899,  0.3017,  0.2095]],\n",
      "\n",
      "         [[-0.1149, -0.5030, -0.4707],\n",
      "          [ 0.0813, -0.2283, -0.4535],\n",
      "          [ 0.2469, -0.0536, -0.1991]],\n",
      "\n",
      "         [[-0.2040,  0.3841,  0.0018],\n",
      "          [-0.1532, -0.0096,  0.3012],\n",
      "          [ 0.1846, -0.0343, -0.0898]],\n",
      "\n",
      "         [[-0.2761,  0.1145, -0.0323],\n",
      "          [-0.1445,  0.5707, -0.0332],\n",
      "          [ 0.2017,  0.1138, -0.4934]],\n",
      "\n",
      "         [[-0.8600,  0.0364,  0.2116],\n",
      "          [-0.3295,  0.2762,  0.6649],\n",
      "          [ 0.2353, -0.3199, -1.0055]]],\n",
      "\n",
      "\n",
      "        [[[-0.5778, -0.3324,  0.3905],\n",
      "          [-0.3114, -0.0774,  0.1546],\n",
      "          [-0.1606, -0.2505, -0.0312]],\n",
      "\n",
      "         [[ 0.0879,  0.5694,  0.2327],\n",
      "          [-0.5148,  0.3821,  0.0157],\n",
      "          [-0.5978, -0.1693,  0.2803]],\n",
      "\n",
      "         [[-0.5098, -0.1960,  0.1132],\n",
      "          [-0.0743, -0.3413, -0.2445],\n",
      "          [ 0.4024,  0.1533, -0.0147]],\n",
      "\n",
      "         [[ 0.2703,  0.3288, -0.1088],\n",
      "          [-0.1951,  0.2897,  0.2138],\n",
      "          [-0.4334,  0.1604, -0.1183]],\n",
      "\n",
      "         [[ 0.5183,  0.1994, -0.4405],\n",
      "          [ 0.0667, -0.0579,  0.0483],\n",
      "          [ 0.0740, -0.1484, -0.1704]]]])\n",
      "conv2.bias\n",
      "tensor([ 0.1219,  0.5986,  0.0946,  0.0988,  0.4082])\n",
      "fc1.weight\n",
      "tensor([[ 0.0826, -0.0590, -0.0676,  0.0161,  0.1541, -0.1281, -0.0403,\n",
      "          0.7019,  0.0261, -0.0741,  0.1083, -0.4229, -0.0724,  0.0727,\n",
      "          0.0550, -0.1649, -0.0334, -0.0108, -0.0120, -0.1295,  0.0143,\n",
      "          0.0869,  0.0626, -0.7201, -0.0446,  0.1004,  0.0947,  0.1402,\n",
      "         -0.0280, -0.0313,  0.1114,  0.2652, -0.1956,  0.1801,  0.3914,\n",
      "         -0.0633,  0.1226,  0.5206,  0.3039,  0.0572, -0.1573, -0.1811,\n",
      "          0.2371, -0.0748,  0.2694, -0.2651, -0.0807,  0.3467, -0.2414,\n",
      "         -0.0122, -0.0059,  0.0419, -0.1165, -0.0256, -0.1381,  0.0225,\n",
      "          0.1507,  0.1811, -0.0563,  0.6277, -0.1436, -0.0546, -0.0054,\n",
      "         -0.1139, -0.0932, -0.0423,  0.0571,  0.3902, -0.1382,  0.0931,\n",
      "         -0.0550, -0.2878, -0.1171,  0.2544,  0.0131, -0.3582,  0.1410,\n",
      "         -0.1806, -0.2177,  0.0474]])\n",
      "fc1.bias\n",
      "tensor([-0.2972])\n"
     ]
    }
   ],
   "source": [
    "params = net.state_dict()\n",
    "for key, value in params.items():\n",
    "    print(key)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.29721215], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['fc1.bias'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.5791,  1.1537,  1.0077,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "\n",
       "         [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.6774,  1.3424,  1.3536,  ..., -0.0000, -0.0000,  0.0000],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = net.pool0(X_test)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.9896,  2.7273,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0083,  0.0000,  0.0000,  0.3413],\n",
       "          [ 3.0476,  4.1616,  0.6891,  0.0000,  0.0000,  0.0000,  0.5120,\n",
       "            0.1046,  0.0506,  0.0256,  0.0559,  0.3914,  0.2587,  0.8099],\n",
       "          [ 0.4298,  0.0136,  0.0000,  0.0000,  0.0000,  0.0177,  2.2565,\n",
       "            3.5687,  2.2694,  0.0290,  0.0000,  0.6461,  0.4188,  0.5486],\n",
       "          [ 0.2837,  0.1487,  0.0131,  0.0000,  1.3816,  1.3646,  0.9308,\n",
       "            1.7003,  1.4421,  0.3033,  0.0007,  0.8213,  0.7713,  1.3598],\n",
       "          [ 0.8529,  0.3343,  0.0017,  0.0275,  3.2024,  2.9152,  3.2534,\n",
       "            1.6996,  0.0464,  0.7146,  0.3012,  2.0293,  1.7992,  1.7493],\n",
       "          [ 0.1673,  0.7290,  0.0984,  0.5467,  2.5228,  2.6183,  1.9869,\n",
       "            1.4354,  1.0777,  2.1503,  1.3492,  1.9878,  1.6343,  0.8734],\n",
       "          [ 0.0161,  1.4348,  1.2058,  1.2815,  0.0603,  0.6792,  1.7345,\n",
       "            2.1348,  1.8072,  1.8675,  1.6613,  0.0859,  0.7999,  1.3455],\n",
       "          [ 0.3000,  2.2074,  2.1229,  1.9502,  0.4755,  0.3724,  2.6843,\n",
       "            3.4154,  2.1598,  1.7008,  3.0519,  0.3541,  0.6708,  2.8422],\n",
       "          [ 0.4133,  1.9127,  3.2532,  0.5471,  0.2410,  0.0121,  0.8759,\n",
       "            2.4240,  1.9827,  0.7485,  1.0200,  1.2391,  1.2995,  2.3155],\n",
       "          [ 0.2745,  0.6818,  2.9252,  0.4011,  0.2463,  1.0568,  1.2936,\n",
       "            1.2870,  0.9294,  3.5114,  0.1068,  0.8307,  3.0491,  4.3835],\n",
       "          [ 2.0435,  1.5727,  3.7328,  0.7964,  0.7708,  4.0634,  4.0473,\n",
       "            3.1453,  1.2100,  3.6900,  0.9162,  1.0685,  2.9083,  2.2860],\n",
       "          [ 1.9500,  2.1991,  2.8368,  0.9641,  0.2934,  2.4250,  3.1760,\n",
       "            1.5339,  0.9160,  1.0116,  0.1926,  0.1993,  1.3248,  0.8825],\n",
       "          [ 0.1588,  0.3501,  0.4535,  0.4057,  0.3092,  0.6970,  0.8075,\n",
       "            0.0535,  0.0000,  0.0012,  0.0000,  0.0000,  0.1934,  0.4359],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0063,  0.0000,  0.0000,  0.0037,\n",
       "            0.0040,  0.0000,  0.0000,  0.0000,  0.0000,  0.0294,  0.2409]],\n",
       "\n",
       "         [[ 0.2551,  0.1521,  1.0925,  0.4292,  0.4260,  0.4220,  0.4233,\n",
       "            0.4220,  0.4220,  0.3655,  0.3801,  0.4761,  0.4817,  0.1976],\n",
       "          [ 1.3951,  0.6283,  0.5767,  0.4219,  0.4225,  0.4077,  0.1426,\n",
       "            0.1897,  0.2163,  0.5347,  0.4438,  0.2734,  0.6492,  0.6718],\n",
       "          [ 1.3811,  0.4942,  0.4650,  0.4220,  0.3879,  0.3896,  0.7177,\n",
       "            0.7862,  0.4550,  1.0386,  0.4482,  0.2764,  1.3480,  0.6996],\n",
       "          [ 0.5000,  1.0790,  0.7536,  0.4220,  0.0045,  0.4241,  0.8943,\n",
       "            0.4778,  1.6221,  1.2600,  0.3984,  0.2391,  0.4559,  0.1282],\n",
       "          [ 0.6931,  0.6481,  0.4890,  0.3285,  0.0043,  1.4541,  0.6257,\n",
       "            1.0865,  0.9635,  0.4744,  0.4257,  0.1041,  0.7444,  1.1130],\n",
       "          [ 0.3900,  0.1093,  0.4462,  0.2954,  1.2048,  1.2715,  1.0268,\n",
       "            1.3259,  0.9586,  0.9780,  0.8986,  1.1699,  0.9853,  0.4909],\n",
       "          [ 0.6851,  0.1770,  1.0943,  0.6269,  1.4058,  0.6678,  0.2354,\n",
       "            0.3302,  0.5760,  0.3752,  0.5382,  1.2575,  0.0176,  0.0625],\n",
       "          [ 0.9241,  0.2769,  0.6891,  0.8967,  1.2360,  1.2852,  1.1359,\n",
       "            1.4113,  1.2882,  1.4385,  1.1878,  1.5407,  0.3144,  0.5974],\n",
       "          [ 0.7890,  0.6771,  0.3834,  1.1182,  0.8158,  0.5417,  0.8053,\n",
       "            0.8920,  0.7033,  0.3378,  0.7641,  0.3991,  0.1228,  0.0506],\n",
       "          [ 1.1923,  1.3873,  0.5386,  0.9858,  0.0754,  0.0386,  0.0038,\n",
       "            0.0943,  1.1962,  0.0497,  1.0302,  0.1016,  0.3541,  1.0659],\n",
       "          [ 0.3804,  0.9137,  0.6610,  0.5658,  0.3011,  0.4385,  1.1382,\n",
       "            0.6867,  0.9642,  1.4540,  0.9100,  0.0431,  0.6687,  1.2700],\n",
       "          [ 1.1239,  1.7133,  1.2332,  0.3734,  0.3893,  1.2995,  1.8547,\n",
       "            1.2335,  0.1949,  0.8899,  0.7465,  0.6954,  0.7479,  1.0924],\n",
       "          [ 0.9216,  0.9465,  0.7271,  0.7749,  0.8257,  0.5464,  0.6337,\n",
       "            0.2127,  0.0416,  0.1603,  0.1426,  0.0834,  0.5735,  0.8440],\n",
       "          [ 0.3830,  0.4319,  0.5114,  0.4893,  0.3982,  0.4188,  0.4347,\n",
       "            0.4769,  0.2904,  0.2384,  0.2564,  0.3420,  0.4640,  0.5380]],\n",
       "\n",
       "         [[ 2.1702,  2.2878,  0.0244,  0.2833,  0.2797,  0.2821,  0.2803,\n",
       "            0.2814,  0.2814,  0.3439,  0.3796,  0.2743,  0.2182,  1.0096],\n",
       "          [ 1.9400,  3.0583,  0.3388,  0.2815,  0.2808,  0.3001,  1.4689,\n",
       "            0.4343,  0.3652,  0.2936,  0.4263,  0.9371,  0.5075,  1.3326],\n",
       "          [ 0.2968,  0.2525,  0.2433,  0.2814,  0.3263,  0.4472,  2.3125,\n",
       "            2.5293,  1.5004,  0.0079,  0.2848,  1.1917,  0.6014,  1.4204],\n",
       "          [ 0.3719,  0.2173,  0.1181,  0.2814,  2.2387,  1.1810,  0.8512,\n",
       "            1.4512,  0.9946,  0.2908,  0.2679,  1.4360,  0.7316,  1.6705],\n",
       "          [ 1.0158,  0.5207,  0.1306,  0.4435,  3.8605,  1.4904,  2.4424,\n",
       "            0.9027,  0.2748,  1.1820,  0.5417,  2.2603,  1.0093,  1.2564],\n",
       "          [ 0.5519,  1.2350,  0.3151,  0.8420,  2.0706,  1.6732,  1.6613,\n",
       "            0.9721,  0.9548,  1.7300,  0.8494,  1.5124,  1.0079,  1.1166],\n",
       "          [ 0.2257,  1.9632,  0.8900,  1.0420,  0.0429,  1.1625,  1.9613,\n",
       "            1.7556,  1.1859,  1.5770,  1.3604,  0.1124,  1.3498,  1.8427],\n",
       "          [ 0.4421,  2.4695,  1.4662,  1.2275,  0.2437,  0.6310,  2.5222,\n",
       "            2.3641,  1.1476,  1.1039,  2.4260,  0.0915,  1.1511,  2.5909],\n",
       "          [ 0.6002,  2.0583,  2.4960,  0.0594,  0.4679,  0.3041,  1.1798,\n",
       "            2.1157,  1.6296,  0.6671,  1.2535,  1.1639,  2.0827,  2.3463],\n",
       "          [ 0.0960,  1.0151,  2.6855,  0.0000,  1.0692,  1.8609,  1.2660,\n",
       "            1.2588,  1.0043,  3.3246,  0.0103,  1.7516,  3.0927,  2.9849],\n",
       "          [ 1.1915,  1.4463,  2.6349,  0.1431,  1.2786,  3.8344,  2.3315,\n",
       "            1.7104,  0.6190,  3.1381,  0.1597,  1.8315,  2.4118,  1.6204],\n",
       "          [ 1.5476,  1.6060,  2.1079,  0.6564,  0.7467,  2.2166,  1.9562,\n",
       "            0.7565,  1.0012,  1.1061,  0.0829,  0.7216,  1.6580,  0.7915],\n",
       "          [ 0.3953,  0.6228,  0.6958,  0.5047,  0.5144,  0.9552,  0.9422,\n",
       "            0.1643,  0.2168,  0.2017,  0.0028,  0.0892,  0.7278,  0.5857],\n",
       "          [ 0.2569,  0.3164,  0.3226,  0.3302,  0.3084,  0.3236,  0.3454,\n",
       "            0.3179,  0.2256,  0.2111,  0.2146,  0.2053,  0.3457,  0.5886]],\n",
       "\n",
       "         [[ 0.7351,  0.7269,  1.0738,  0.7484,  0.7540,  0.7510,  0.7515,\n",
       "            0.7510,  0.7510,  0.7389,  0.6487,  0.7743,  0.8651,  0.6306],\n",
       "          [ 4.4176,  6.3757,  2.5438,  0.7509,  0.7510,  0.7818,  0.5212,\n",
       "            0.2407,  0.2606,  0.4990,  0.9423,  0.6100,  0.3442,  0.7337],\n",
       "          [ 1.7163,  0.8167,  0.6978,  0.7510,  0.7505,  0.6558,  0.3838,\n",
       "            2.5485,  1.8010,  1.0723,  0.7546,  0.7606,  1.4689,  0.3884],\n",
       "          [ 0.4665,  0.5927,  0.8453,  0.7510,  0.2780,  0.1617,  0.9212,\n",
       "            1.3339,  2.0870,  1.8774,  0.7225,  0.3913,  0.5655,  0.1624],\n",
       "          [ 1.3897,  1.1122,  0.7739,  0.6698,  0.2463,  2.1883,  1.5936,\n",
       "            1.9164,  1.0533,  0.3008,  0.2948,  0.2808,  1.4285,  1.9512],\n",
       "          [ 0.7530,  0.8736,  0.5274,  0.4209,  1.6996,  3.4788,  1.9293,\n",
       "            1.8830,  0.2678,  0.9662,  1.4407,  1.7626,  2.4228,  1.2413],\n",
       "          [ 0.6367,  0.3865,  0.7479,  0.6793,  1.0892,  0.7602,  0.6019,\n",
       "            0.2716,  0.8707,  1.4737,  0.8903,  1.4200,  1.4257,  0.1453],\n",
       "          [ 1.1753,  0.5836,  1.0862,  1.3973,  1.2905,  0.7759,  1.2021,\n",
       "            2.5960,  2.4466,  1.6457,  1.6590,  1.4561,  0.7313,  1.1160],\n",
       "          [ 1.0081,  0.9887,  2.0585,  1.9359,  1.3487,  0.3247,  0.5200,\n",
       "            1.4299,  1.7094,  1.0091,  1.0398,  1.9208,  0.3568,  0.5164],\n",
       "          [ 0.8675,  1.1993,  1.3901,  1.4402,  0.3841,  0.1482,  0.1961,\n",
       "            0.1895,  0.6845,  1.6745,  0.7168,  1.0854,  0.2428,  2.4467],\n",
       "          [ 1.6164,  0.1193,  1.3867,  2.1717,  0.5294,  1.1400,  2.6730,\n",
       "            2.4190,  1.3959,  1.4511,  3.6297,  0.3547,  1.7007,  2.3757],\n",
       "          [ 2.1985,  2.2196,  3.5011,  2.6760,  0.3400,  1.0796,  2.8381,\n",
       "            3.1990,  1.9582,  1.6248,  2.0472,  1.0436,  2.1935,  1.6826],\n",
       "          [ 0.9786,  0.9702,  1.1615,  1.1821,  1.1577,  1.5350,  1.7636,\n",
       "            1.4961,  0.8438,  1.1615,  0.6214,  0.0462,  0.1013,  0.6508],\n",
       "          [ 0.6962,  0.7653,  0.7784,  0.8291,  0.8083,  0.7999,  0.8404,\n",
       "            0.7820,  0.8051,  0.7105,  0.6447,  0.6678,  0.7537,  1.1307]],\n",
       "\n",
       "         [[ 0.0510,  0.6798,  1.0861,  0.5818,  0.5838,  0.5847,  0.5862,\n",
       "            0.5875,  0.5875,  0.5113,  0.5124,  0.4745,  0.5642,  0.2394],\n",
       "          [ 0.0000,  0.0000,  0.2148,  0.5878,  0.5884,  0.5440,  0.1985,\n",
       "            0.5463,  0.7998,  0.5689,  0.3065,  0.2387,  1.0065,  0.3139],\n",
       "          [ 0.0000,  0.4828,  0.5563,  0.5875,  0.5292,  0.4629,  0.0000,\n",
       "            0.0000,  0.1330,  0.4308,  0.5879,  0.1646,  0.7867,  0.2400],\n",
       "          [ 0.1644,  0.3449,  0.4509,  0.5875,  0.0000,  0.0879,  0.1343,\n",
       "            0.0000,  0.0303,  0.0607,  0.6152,  0.0906,  0.1457,  0.0686],\n",
       "          [ 0.0412,  0.1113,  0.5461,  0.4873,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0747,  0.2895,  0.0926,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.3747,  0.0000,  0.0461,  0.2707,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.1026],\n",
       "          [ 0.5832,  0.0000,  0.0000,  0.0000,  0.0890,  0.0677,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.1807,  0.0080,  0.0292],\n",
       "          [ 0.0948,  0.0000,  0.0000,  0.0000,  0.1789,  0.1675,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.0000,  0.0852,  0.1474,  0.0000],\n",
       "          [ 0.1784,  0.0000,  0.0000,  0.1824,  0.2240,  0.5834,  0.3313,\n",
       "            0.0000,  0.0000,  0.4871,  0.5450,  0.0434,  0.0926,  0.0000],\n",
       "          [ 0.0522,  0.0000,  0.0000,  0.4788,  0.2405,  0.2184,  0.1174,\n",
       "            0.0102,  0.0656,  0.0000,  1.4971,  0.2373,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.2372,  0.0000,  0.0000,  0.0000,\n",
       "            0.0000,  0.0000,  0.0000,  0.1245,  0.0798,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0273,  0.0000,  0.0000,\n",
       "            0.0543,  0.1220,  0.3066,  0.7843,  0.5052,  0.0000,  0.0000],\n",
       "          [ 0.2223,  0.1408,  0.0105,  0.0040,  0.1010,  0.0641,  0.0033,\n",
       "            0.6508,  1.3773,  1.6330,  2.1952,  1.9565,  0.4345,  0.0728],\n",
       "          [ 0.6583,  0.5834,  0.5483,  0.5275,  0.5536,  0.5587,  0.5294,\n",
       "            0.5005,  0.6053,  0.7287,  0.7063,  0.6690,  0.5857,  0.3269]]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = net.pool1(F.relu(net.conv1(mat)))\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 1.7245,  2.9144,  2.0041,  1.7712],\n",
       "          [ 2.3932,  1.3243,  2.3043,  1.8784],\n",
       "          [ 1.4030,  2.8169,  1.7986,  2.3987],\n",
       "          [ 1.6967,  1.3228,  1.4515,  2.0170]],\n",
       "\n",
       "         [[ 2.6577,  1.0501,  1.6945,  1.1951],\n",
       "          [ 0.7797,  1.7055,  0.6594,  0.9934],\n",
       "          [ 1.5289,  0.6554,  1.5661,  1.0234],\n",
       "          [ 1.6763,  1.5860,  1.6955,  1.1643]],\n",
       "\n",
       "         [[ 0.3246,  0.6026,  0.1709,  0.3178],\n",
       "          [ 0.4125,  0.6620,  0.0015,  0.2274],\n",
       "          [ 0.4246,  0.6517,  0.1932,  0.7214],\n",
       "          [ 0.2037,  0.8563,  0.1220,  0.2548]],\n",
       "\n",
       "         [[ 0.4498,  0.6473,  0.6233,  0.2590],\n",
       "          [ 0.4052,  1.1604,  0.7552,  1.0241],\n",
       "          [ 1.0047,  0.7187,  1.1670,  1.3284],\n",
       "          [ 0.9126,  0.9888,  0.5209,  0.2752]],\n",
       "\n",
       "         [[ 0.1560,  0.6472,  0.0675,  0.3956],\n",
       "          [ 0.5628,  0.0311,  0.0000,  0.0269],\n",
       "          [ 0.0000,  0.5598,  0.0000,  0.0000],\n",
       "          [ 0.0566,  0.2672,  0.0352,  0.5312]]]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = net.pool2(F.relu(net.conv2(mat)))\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7245,  2.9144,  2.0041,  1.7712,  2.3932,  1.3243,  2.3043,\n",
       "          1.8784,  1.4030,  2.8169,  1.7986,  2.3987,  1.6967,  1.3228,\n",
       "          1.4515,  2.0170,  2.6577,  1.0501,  1.6945,  1.1951,  0.7797,\n",
       "          1.7055,  0.6594,  0.9934,  1.5289,  0.6554,  1.5661,  1.0234,\n",
       "          1.6763,  1.5860,  1.6955,  1.1643,  0.3246,  0.6026,  0.1709,\n",
       "          0.3178,  0.4125,  0.6620,  0.0015,  0.2274,  0.4246,  0.6517,\n",
       "          0.1932,  0.7214,  0.2037,  0.8563,  0.1220,  0.2548,  0.4498,\n",
       "          0.6473,  0.6233,  0.2590,  0.4052,  1.1604,  0.7552,  1.0241,\n",
       "          1.0047,  0.7187,  1.1670,  1.3284,  0.9126,  0.9888,  0.5209,\n",
       "          0.2752,  0.1560,  0.6472,  0.0675,  0.3956,  0.5628,  0.0311,\n",
       "          0.0000,  0.0269,  0.0000,  0.5598,  0.0000,  0.0000,  0.0566,\n",
       "          0.2672,  0.0352,  0.5312]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = mat.view(-1, 80)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5504]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = net.fc1(mat)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.2570]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat + month_mean[2, row[1], row[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测（92°W，0°）今年3月份海温（真实值为27.26599℃）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.5039]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = np.zeros((1, 2, 89, 180), dtype=np.float32)\n",
    "X_test[0, 0] = SST[-3, 0]\n",
    "X_test[0, 1] = SST[-2, 0]\n",
    "X_test = torch.from_numpy(X_test)\n",
    "\n",
    "y_fore = net(X_test) + month_mean[2, row[1], row[0]]\n",
    "y_fore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 27.5720]])\n",
      "tensor([[ 29.1317]])\n",
      "tensor([[ 27.6299]])\n",
      "tensor([[ 29.9275]])\n",
      "tensor([[ 27.4515]])\n",
      "tensor([[ 3.9730]])\n",
      "tensor([[ 24.6470]])\n",
      "tensor([[ 19.6922]])\n",
      "tensor([[ 16.1494]])\n",
      "tensor([[ 5.9008]])\n"
     ]
    }
   ],
   "source": [
    "for row in loc:\n",
    "    y_train = SST[2:-15, :, row[1], row[0]]\n",
    "    y_train = torch.from_numpy(y_train)\n",
    "    y_valid = SST[-15:-3, :, row[1], row[0]]\n",
    "    y_valid = torch.from_numpy(y_valid)\n",
    "    \n",
    "    net = Net()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    for epoch in range(1000):  # loop over the dataset multiple times\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(X_train)\n",
    "        loss = criterion(outputs, y_train)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        outputs_valid = net(X_valid)\n",
    "        loss_valid = criterion(outputs_valid, y_valid)\n",
    "    \n",
    "    print(net(X_test) + month_mean[2, row[1], row[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classfication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据上传到百度网盘 内涝等级识别 文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler #only can be imported above pytorch0.2.0\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "image_datasets = datasets.ImageFolder('train',\n",
    "                                      data_transforms)\n",
    "\n",
    "test_datasets = datasets.ImageFolder('test',\n",
    "                                      data_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weights_for_balanced_classes(images, nclasses):                        \n",
    "    count = [0] * nclasses\n",
    "    for item in images:\n",
    "        count[item[1]] += 1\n",
    "    weight_per_class = [0.] * nclasses\n",
    "    N = float(sum(count))\n",
    "    for i in range(nclasses):\n",
    "        weight_per_class[i] = N/float(count[i])\n",
    "    weight = [0] * len(images)\n",
    "    for idx, val in enumerate(images):\n",
    "        weight[idx] = weight_per_class[val[1]]\n",
    "    return weight\n",
    "\n",
    "weight = make_weights_for_balanced_classes(image_datasets, 5)\n",
    "\n",
    "sampler = torch.utils.data.sampler.WeightedRandomSampler(weight, len(weight))\n",
    "\n",
    "dataloders = torch.utils.data.DataLoader(image_datasets,\n",
    "                                         batch_size=1,\n",
    "                                         sampler = sampler,\n",
    "                                         num_workers=0)\n",
    "\n",
    "datatest = torch.utils.data.DataLoader(test_datasets,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = models.resnet18(pretrained=True)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(25):  # loop over the dataset multiple times\n",
    "    scheduler.step()\n",
    "    i = 0\n",
    "    loss_sum = 0\n",
    "    for data in dataloders:\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_sum += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "\n",
    "            print(i, loss_sum / 1000)\n",
    "\n",
    "            loss_sum = 0\n",
    "\n",
    "    correct = np.zeros((5, 5))\n",
    "    for data in datatest:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct[int(labels), int(predicted)] += 1\n",
    "    for row in correct:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python36\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.1513543e-05 9.9989283e-01 4.3832726e-05 1.4410913e-06 3.7497728e-07] 1\n",
      "[1.9392589e-06 9.7590164e-06 1.0727131e-05 1.8801609e-04 9.9978954e-01] 4\n",
      "[1.6476604e-05 3.4625569e-04 9.9959826e-01 3.7069316e-05 1.9144195e-06] 2\n",
      "[1.8656605e-05 2.6466485e-04 2.7656858e-04 4.3591481e-01 5.6352532e-01] 3\n",
      "[9.9962622e-01 3.5922590e-04 7.4621644e-06 5.9296272e-06 1.1686778e-06] 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler #only can be imported above pytorch0.2.0\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "test_datasets = datasets.ImageFolder('test_new',\n",
    "                                      data_transforms)\n",
    "\n",
    "datatest = torch.utils.data.DataLoader(test_datasets,\n",
    "                                         batch_size=1,\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=0)\n",
    "\n",
    "net = models.resnet18(pretrained=True)\n",
    "num_ftrs = net.fc.in_features\n",
    "net.fc = nn.Linear(num_ftrs, 5)\n",
    "\n",
    "net.load_state_dict(torch.load('test.pkl'), strict=False)\n",
    "\n",
    "for data in datatest:\n",
    "    inputs, labels = data\n",
    "    outputs = F.softmax(net(inputs))\n",
    "    print(outputs.detach().numpy()[0], int(labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
